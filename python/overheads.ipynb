{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To force reloading. Useful when editing python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "from processor_new import *\n",
    "from overhead import *\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With pretenuring\n",
      "\n",
      "Overhead for 1/1000:  1.1217817361038898  std:  0.001058019049360465\n",
      "Overhead for 1/100:  1.078106935974211  std:  0.0014107835236937742\n",
      "Overhead for 1/2:  1.6455673748019648  std:  0.012204825322310237\n",
      "Overhead for 1:  2.296450352028676  std:  0.03147629821347439\n",
      "\n",
      "NO pretenuring\n",
      "\n",
      "Overhead for 1/1000:  2.2701108997375843  std:  0.012914888252197087\n",
      "Overhead for 1/100:  2.248738385500456  std:  0.01900334869836985\n",
      "Overhead for 1/2:  3.8424648735938733  std:  0.019868147832235623\n",
      "Overhead for 1:  5.802099326527243  std:  0.017575526046920285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "illi_bench = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/profiler-data/illiDFPlusHG.csv', comment='#', sep ='\\t')\n",
    "illi_bench_no_prete = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/noPretenuring.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "print('With pretenuring')\n",
    "baseline_mean = print_overheads(illi_bench, 'IllBenchmark', 'BaselineBenchmark')\n",
    "\n",
    "print()\n",
    "print('NO pretenuring')\n",
    "print_overheads(illi_bench_no_prete, 'IllBenchmarkNoPretenuring', 0, False, baseline_mean)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honey Ginger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overhead for 1/1000:  1.8290150330388717  std:  0.05653724978154955\n",
      "Overhead for 1/100:  2.25225072910943  std:  0.06096557419881308\n",
      "Overhead for 1/2:  2.492892063626245  std:  0.10104244677319554\n",
      "Overhead for 1:  3.1948236752750887  std:  0.17048403733024678\n"
     ]
    }
   ],
   "source": [
    "illi_bench_hg = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/profiler-data/illiDFPlusHG.csv', comment='#', sep ='\\t')\n",
    "\n",
    "print_overheads(illi_bench_hg, 'IllBenchmarkHG', 'BaselineBenchmarkHG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PRETENURING\n",
      "Overhead for 1/1000:  8.379559575637177  std:  0.5421194329612875\n",
      "Overhead for 1/100:  7.795849359651718  std:  0.4604052516737718\n",
      "Overhead for 1/2:  nan  std:  nan\n",
      "Overhead for 1:  9.250686842216492  std:  0.6228585815961356\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/new-clean/missingInfo2.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'HGBench') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'HGBench') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'HGBench') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'HGBench') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "another_df = illi_bench_hg = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/profiler-data/illiDFPlusHG.csv', comment='#', sep ='\\t')\n",
    "baseline = another_df[ (another_df['suite'] == 'BaselineBenchmarkHG') & (another_df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('NO PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH PRETENURING\n",
      "Overhead for 1/1000:  1.0352219158587566  std:  0.0007514567036934706\n",
      "Overhead for 1/100:  1.0350440273948234  std:  0.0007799580088668159\n",
      "Overhead for 1/2:  1.0351626197041122  std:  0.001072515125748853\n",
      "Overhead for 1:  1.036407838951644  std:  0.0010362943058505256\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/noPreteBenchBlocMoose.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBloc') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBloc') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBloc') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBloc') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "another_df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineBlocPlusMD.csv', comment='#', sep ='\\t')\n",
    "baseline = another_df[ (another_df['suite'] == 'BaselineBloc') & (another_df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('WITH PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PRETENURING\n",
      "Overhead for 1/1000:  1.0404696255447834  std:  0.0006769762475383681\n",
      "Overhead for 1/100:  1.0400249043849505  std:  0.0008786420991260354\n",
      "Overhead for 1/2:  1.040736458240683  std:  0.0012868061739308846\n",
      "Overhead for 1:  1.0401731447715614  std:  0.002162761564866125\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/noPreteBenchBlocMoose.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBlocNoPret') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBlocNoPret') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBlocNoPret') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllBlocBench') & (df['executor'] == 'PharoExecutorBlocNoPret') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "another_df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineBlocPlusMD.csv', comment='#', sep ='\\t')\n",
    "baseline = another_df[ (another_df['suite'] == 'BaselineBloc') & (another_df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('NO PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH PRETENURING\n",
      "Overhead for 1/1000:  1.0917975266033937  std:  0.002388296759466901\n",
      "Overhead for 1/100:  1.1026459591601956  std:  0.0035070562899764987\n",
      "Overhead for 1/2:  1.5990394017831464  std:  0.0076407025504145725\n",
      "Overhead for 1:  2.017704918032787  std:  0.034845662272537443\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/noPreteBenchBlocMoose.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMoose') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMoose') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMoose') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMoose') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "another_df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineMoose.csv', comment='#', sep ='\\t')\n",
    "baseline = another_df[ (another_df['suite'] == 'BaselineMoose') & (another_df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('WITH PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PRETENURING\n",
      "Overhead for 1/1000:  2.0047857348288756  std:  0.0047656174638351905\n",
      "Overhead for 1/100:  2.019430543572045  std:  0.006630813839832198\n",
      "Overhead for 1/2:  2.6019442047742305  std:  0.008720923538744749\n",
      "Overhead for 1:  3.2223641069887834  std:  0.016800295473225965\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/noPreteBenchBlocMoose.csv', comment='#', sep ='\\t')\n",
    "\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMooseNoPret') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMooseNoPret') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMooseNoPret') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllMooseBench') & (df['executor'] == 'PharoExecutorMooseNoPret') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "another_df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineMoose.csv', comment='#', sep ='\\t')\n",
    "baseline = another_df[ (another_df['suite'] == 'BaselineMoose') & (another_df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('NO PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re:Mobidyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH PRETENURING\n",
      "Overhead for 1/1000:  1.2449052374159364  std:  0.05469192632600278\n",
      "Overhead for 1/100:  1.3125636845323008  std:  0.013029799281174382\n",
      "Overhead for 1/2:  1.5357652333401264  std:  0.004590236894582684\n",
      "Overhead for 1:  2.06490727532097  std:  0.00504677317000177\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineBlocPlusMD.csv', comment='#', sep ='\\t')\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobi') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobi') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobi') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobi') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "baseline = df[ (df['suite'] == 'BaselineMobi') & (df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('WITH PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PRETENURING\n",
      "Overhead for 1/1000:  9.01609944976564  std:  0.08735634943202368\n",
      "Overhead for 1/100:  9.149174648461381  std:  0.1274005208761759\n",
      "Overhead for 1/2:  13.76757693091502  std:  0.11019139387576854\n",
      "Overhead for 1:  32.312869370287345  std:  2.1751643048906963\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sebastian/Documents/PharoImages/illi-df-hg-clean/baselineBlocPlusMD.csv', comment='#', sep ='\\t')\n",
    "\n",
    "# Profiler mean values\n",
    "df_01 = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobiNoPret') & (df['inputSize'] == '1/1000') & (df['criterion'] == 'total') ]\n",
    "df_mean_01 = df_01['value'].mean()\n",
    "df_std_01 = df_01['value'].std()\n",
    "\n",
    "df_1_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobiNoPret') & (df['inputSize'] == '1/100') & (df['criterion'] == 'total') ]\n",
    "df_mean_1 = df_1_df['value'].mean()\n",
    "df_std_1 = df_1_df['value'].std()\n",
    "\n",
    "df_50_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobiNoPret') & (df['inputSize'] == '1/2') & (df['criterion'] == 'total') ]\n",
    "df_mean_50 = df_50_df['value'].mean()\n",
    "df_std_50 = df_50_df['value'].std()\n",
    "\n",
    "df_100_df = df[ (df['suite'] == 'IllMobiBench') & (df['executor'] == 'PharoExecutorMobiNoPret') & (df['inputSize'] == '1') & (df['criterion'] == 'total') ]\n",
    "df_mean_100 = df_100_df['value'].mean()\n",
    "df_std_100 = df_100_df['value'].std()\n",
    "\n",
    "# baseline\n",
    "baseline = df[ (df['suite'] == 'BaselineMobi') & (df['criterion'] == 'total') ]\n",
    "baseline_mean = baseline['value'].mean()\n",
    "\n",
    "\n",
    "# overhead using mean\n",
    "print('NO PRETENURING')\n",
    "print('Overhead for 1/1000: ', str(df_mean_01 / baseline_mean), ' std: ', str(df_std_01 / baseline_mean))\n",
    "print('Overhead for 1/100: ', str(df_mean_1  / baseline_mean), ' std: ', str(df_std_1 / baseline_mean))\n",
    "print('Overhead for 1/2: ', str(df_mean_50  / baseline_mean), ' std: ', str(df_std_50 / baseline_mean))\n",
    "print('Overhead for 1: ',  str(df_mean_100  / baseline_mean), ' std: ', str(df_std_100 / baseline_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
